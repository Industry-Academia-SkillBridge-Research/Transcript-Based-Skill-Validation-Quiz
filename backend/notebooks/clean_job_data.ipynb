{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36baf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85fd633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe08321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 194 job records\n",
      "\n",
      "Columns: ['job_id', 'title', 'company', 'location', 'posted_date', 'job_url', 'scraped_at', 'description', 'seniority_level', 'employment_type', 'job_function', 'industries', 'skills', 'role_tag', 'role_key', 'job_role_id']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>job_url</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>description</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_function</th>\n",
       "      <th>industries</th>\n",
       "      <th>skills</th>\n",
       "      <th>role_tag</th>\n",
       "      <th>role_key</th>\n",
       "      <th>job_role_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lead-ai-ml-engineer-%2B-agent-orchestration-1m...</td>\n",
       "      <td>Lead AI/ML Engineer + Agent Orchestration (1M ...</td>\n",
       "      <td>codax</td>\n",
       "      <td>Colombo, Western Province, Sri Lanka</td>\n",
       "      <td>2025-11-26</td>\n",
       "      <td>https://lk.linkedin.com/jobs/view/lead-ai-ml-e...</td>\n",
       "      <td>2025-11-27T16:50:00.423993</td>\n",
       "      <td>Role: Lead AI/ML Engineer + Agent Orchestratio...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Engineering and Information Technology</td>\n",
       "      <td>Marketing Services</td>\n",
       "      <td>Python | FastAPI | LangChain | LlamaIndex | Po...</td>\n",
       "      <td>AIML</td>\n",
       "      <td>ai_ml_engineer</td>\n",
       "      <td>AIML_20251127_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>machine-learning-engineer-python-ifs-loops-at-...</td>\n",
       "      <td>Machine Learning Engineer (Python)- IFS Loops</td>\n",
       "      <td>IFS</td>\n",
       "      <td>Colombo, Western Province, Sri Lanka</td>\n",
       "      <td>2025-11-20</td>\n",
       "      <td>https://lk.linkedin.com/jobs/view/machine-lear...</td>\n",
       "      <td>2025-11-27T16:50:00.424007</td>\n",
       "      <td>IFS is a billion-dollar revenue company with 7...</td>\n",
       "      <td>Internship</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Human Resources</td>\n",
       "      <td>IT Services and IT Consulting</td>\n",
       "      <td>Python | Flask | FastAPI | scikit-learn | Tens...</td>\n",
       "      <td>AIML</td>\n",
       "      <td>ai_ml_engineer</td>\n",
       "      <td>AIML_20251127_003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ai-engineer-at-heymilo-ai-4323227897</td>\n",
       "      <td>AI Engineer</td>\n",
       "      <td>HeyMilo AI</td>\n",
       "      <td>Colombo, Western Province, Sri Lanka</td>\n",
       "      <td>2025-11-19</td>\n",
       "      <td>https://lk.linkedin.com/jobs/view/ai-engineer-...</td>\n",
       "      <td>2025-11-27T16:50:00.424010</td>\n",
       "      <td>Location: Remote Sri LankaType: Full-TimeAt He...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Engineering and Information Technology</td>\n",
       "      <td>Software Development</td>\n",
       "      <td>Python | FastAPI | LLM | RAG | NLP | Deep Lear...</td>\n",
       "      <td>AIML</td>\n",
       "      <td>ai_ml_engineer</td>\n",
       "      <td>AIML_20251127_005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>full-stack-ai-application-engineer-at-prismeti...</td>\n",
       "      <td>Full-Stack AI Application Engineer</td>\n",
       "      <td>PRISMETIC</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>2025-11-22</td>\n",
       "      <td>https://lk.linkedin.com/jobs/view/full-stack-a...</td>\n",
       "      <td>2025-11-27T16:50:00.424011</td>\n",
       "      <td>Join Our AI Innovation Team as a Full-Stack AI...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Engineering and Information Technology</td>\n",
       "      <td>IT Services and IT Consulting</td>\n",
       "      <td>Python | FastAPI | Django | Flask | JavaScript...</td>\n",
       "      <td>AIML</td>\n",
       "      <td>ai_ml_engineer</td>\n",
       "      <td>AIML_20251127_008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>senior-ai-augmented-software-engineer-at-dijit...</td>\n",
       "      <td>Senior AI-Augmented Software Engineer</td>\n",
       "      <td>Dijital Team</td>\n",
       "      <td>Colombo, Western Province, Sri Lanka</td>\n",
       "      <td>2025-11-26</td>\n",
       "      <td>https://lk.linkedin.com/jobs/view/senior-ai-au...</td>\n",
       "      <td>2025-11-27T16:50:00.424013</td>\n",
       "      <td>Our Tech Culture: Prefer developers who use AI...</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Engineering and Information Technology</td>\n",
       "      <td>Transportation, Logistics, Supply Chain and St...</td>\n",
       "      <td>TypeScript | React | Node.js | REST API | GitH...</td>\n",
       "      <td>AIML</td>\n",
       "      <td>ai_ml_engineer</td>\n",
       "      <td>AIML_20251127_009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              job_id  \\\n",
       "0  lead-ai-ml-engineer-%2B-agent-orchestration-1m...   \n",
       "1  machine-learning-engineer-python-ifs-loops-at-...   \n",
       "2               ai-engineer-at-heymilo-ai-4323227897   \n",
       "3  full-stack-ai-application-engineer-at-prismeti...   \n",
       "4  senior-ai-augmented-software-engineer-at-dijit...   \n",
       "\n",
       "                                               title       company  \\\n",
       "0  Lead AI/ML Engineer + Agent Orchestration (1M ...         codax   \n",
       "1      Machine Learning Engineer (Python)- IFS Loops           IFS   \n",
       "2                                        AI Engineer    HeyMilo AI   \n",
       "3                 Full-Stack AI Application Engineer     PRISMETIC   \n",
       "4              Senior AI-Augmented Software Engineer  Dijital Team   \n",
       "\n",
       "                               location posted_date  \\\n",
       "0  Colombo, Western Province, Sri Lanka  2025-11-26   \n",
       "1  Colombo, Western Province, Sri Lanka  2025-11-20   \n",
       "2  Colombo, Western Province, Sri Lanka  2025-11-19   \n",
       "3                             Sri Lanka  2025-11-22   \n",
       "4  Colombo, Western Province, Sri Lanka  2025-11-26   \n",
       "\n",
       "                                             job_url  \\\n",
       "0  https://lk.linkedin.com/jobs/view/lead-ai-ml-e...   \n",
       "1  https://lk.linkedin.com/jobs/view/machine-lear...   \n",
       "2  https://lk.linkedin.com/jobs/view/ai-engineer-...   \n",
       "3  https://lk.linkedin.com/jobs/view/full-stack-a...   \n",
       "4  https://lk.linkedin.com/jobs/view/senior-ai-au...   \n",
       "\n",
       "                   scraped_at  \\\n",
       "0  2025-11-27T16:50:00.423993   \n",
       "1  2025-11-27T16:50:00.424007   \n",
       "2  2025-11-27T16:50:00.424010   \n",
       "3  2025-11-27T16:50:00.424011   \n",
       "4  2025-11-27T16:50:00.424013   \n",
       "\n",
       "                                         description   seniority_level  \\\n",
       "0  Role: Lead AI/ML Engineer + Agent Orchestratio...  Mid-Senior level   \n",
       "1  IFS is a billion-dollar revenue company with 7...        Internship   \n",
       "2  Location: Remote Sri LankaType: Full-TimeAt He...  Mid-Senior level   \n",
       "3  Join Our AI Innovation Team as a Full-Stack AI...  Mid-Senior level   \n",
       "4  Our Tech Culture: Prefer developers who use AI...    Not Applicable   \n",
       "\n",
       "  employment_type                            job_function  \\\n",
       "0       Full-time  Engineering and Information Technology   \n",
       "1       Full-time                         Human Resources   \n",
       "2       Full-time  Engineering and Information Technology   \n",
       "3       Full-time  Engineering and Information Technology   \n",
       "4       Full-time  Engineering and Information Technology   \n",
       "\n",
       "                                          industries  \\\n",
       "0                                 Marketing Services   \n",
       "1                      IT Services and IT Consulting   \n",
       "2                               Software Development   \n",
       "3                      IT Services and IT Consulting   \n",
       "4  Transportation, Logistics, Supply Chain and St...   \n",
       "\n",
       "                                              skills role_tag        role_key  \\\n",
       "0  Python | FastAPI | LangChain | LlamaIndex | Po...     AIML  ai_ml_engineer   \n",
       "1  Python | Flask | FastAPI | scikit-learn | Tens...     AIML  ai_ml_engineer   \n",
       "2  Python | FastAPI | LLM | RAG | NLP | Deep Lear...     AIML  ai_ml_engineer   \n",
       "3  Python | FastAPI | Django | Flask | JavaScript...     AIML  ai_ml_engineer   \n",
       "4  TypeScript | React | Node.js | REST API | GitH...     AIML  ai_ml_engineer   \n",
       "\n",
       "         job_role_id  \n",
       "0  AIML_20251127_001  \n",
       "1  AIML_20251127_003  \n",
       "2  AIML_20251127_005  \n",
       "3  AIML_20251127_008  \n",
       "4  AIML_20251127_009  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the job data CSV\n",
    "df = pd.read_csv('../data/Job_data.csv')\n",
    "print(f\"Loaded {len(df)} job records\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e00b941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique skills: 582\n",
      "\n",
      "First 50 unique skills:\n",
      "  1. .NET\n",
      "  2. .NET 6\n",
      "  3. .NET Core\n",
      "  4. A/B Testing\n",
      "  5. AI\n",
      "  6. AI Infrastructure\n",
      "  7. AI Systems\n",
      "  8. ALB\n",
      "  9. API\n",
      " 10. API Gateway\n",
      " 11. API Integration\n",
      " 12. APIs\n",
      " 13. AR\n",
      " 14. ASP.NET\n",
      " 15. ASP.NET Core\n",
      " 16. AWS\n",
      " 17. AWS DynamoDB\n",
      " 18. AWS EC2\n",
      " 19. AWS Inferentia\n",
      " 20. AWS RDS\n",
      " 21. AWS S3\n",
      " 22. Agile\n",
      " 23. Airbyte\n",
      " 24. Airflow\n",
      " 25. Airtable\n",
      " 26. Alerting\n",
      " 27. Algorithms\n",
      " 28. Alteryx\n",
      " 29. Amazon Web Services\n",
      " 30. Analytics\n",
      " 31. Android\n",
      " 32. Android Development\n",
      " 33. Android Studio\n",
      " 34. Angular\n",
      " 35. Ansible\n",
      " 36. Ant Design\n",
      " 37. Anthropic\n",
      " 38. Apache Beam\n",
      " 39. Apache Cassandra\n",
      " 40. Apache Kafka\n",
      " 41. Apache Spark\n",
      " 42. Approximate Algorithms\n",
      " 43. ArcGIS\n",
      " 44. ArgoCD\n",
      " 45. Artificial Intelligence\n",
      " 46. Asynchronous Programming\n",
      " 47. AutoGen\n",
      " 48. AutoML\n",
      " 49. Automation\n",
      " 50. Autopilots\n",
      "\n",
      "To see all 582 skills, check the unique_skills variable\n"
     ]
    }
   ],
   "source": [
    "# Get unique skills from the skills column\n",
    "all_skills = set()\n",
    "for skills_str in df['skills'].dropna():\n",
    "    # Split by '|' and add each skill to the set\n",
    "    skills_list = [s.strip() for s in str(skills_str).split('|') if s.strip()]\n",
    "    all_skills.update(skills_list)\n",
    "\n",
    "unique_skills = sorted(all_skills)\n",
    "print(f\"Total unique skills: {len(unique_skills)}\")\n",
    "print(f\"\\nFirst 50 unique skills:\")\n",
    "for i, skill in enumerate(unique_skills[:50], 1):\n",
    "    print(f\"{i:3d}. {skill}\")\n",
    "\n",
    "# Show the full list\n",
    "print(f\"\\nTo see all {len(unique_skills)} skills, check the unique_skills variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5683e95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.NET',\n",
       " '.NET 6',\n",
       " '.NET Core',\n",
       " 'A/B Testing',\n",
       " 'AI',\n",
       " 'AI Infrastructure',\n",
       " 'AI Systems',\n",
       " 'ALB',\n",
       " 'API',\n",
       " 'API Gateway',\n",
       " 'API Integration',\n",
       " 'APIs',\n",
       " 'AR',\n",
       " 'ASP.NET',\n",
       " 'ASP.NET Core',\n",
       " 'AWS',\n",
       " 'AWS DynamoDB',\n",
       " 'AWS EC2',\n",
       " 'AWS Inferentia',\n",
       " 'AWS RDS',\n",
       " 'AWS S3',\n",
       " 'Agile',\n",
       " 'Airbyte',\n",
       " 'Airflow',\n",
       " 'Airtable',\n",
       " 'Alerting',\n",
       " 'Algorithms',\n",
       " 'Alteryx',\n",
       " 'Amazon Web Services',\n",
       " 'Analytics',\n",
       " 'Android',\n",
       " 'Android Development',\n",
       " 'Android Studio',\n",
       " 'Angular',\n",
       " 'Ansible',\n",
       " 'Ant Design',\n",
       " 'Anthropic',\n",
       " 'Apache Beam',\n",
       " 'Apache Cassandra',\n",
       " 'Apache Kafka',\n",
       " 'Apache Spark',\n",
       " 'Approximate Algorithms',\n",
       " 'ArcGIS',\n",
       " 'ArgoCD',\n",
       " 'Artificial Intelligence',\n",
       " 'Asynchronous Programming',\n",
       " 'AutoGen',\n",
       " 'AutoML',\n",
       " 'Automation',\n",
       " 'Autopilots',\n",
       " 'Azure',\n",
       " 'Azure AD',\n",
       " 'Azure AD B2C',\n",
       " 'Azure AI Foundry',\n",
       " 'Azure AI Services',\n",
       " 'Azure API Management',\n",
       " 'Azure Active Directory',\n",
       " 'Azure App Service',\n",
       " 'Azure Application Insights',\n",
       " 'Azure Cosmos DB',\n",
       " 'Azure Data Factory',\n",
       " 'Azure Data Lake',\n",
       " 'Azure DevOps',\n",
       " 'Azure Event Grid',\n",
       " 'Azure Front Door',\n",
       " 'Azure Functions',\n",
       " 'Azure Key Vault',\n",
       " 'Azure Log Analytics',\n",
       " 'Azure Machine Learning',\n",
       " 'Azure OpenAI',\n",
       " 'Azure Pipelines',\n",
       " 'Azure PostgreSQL',\n",
       " 'Azure SQL Database',\n",
       " 'Azure Service Bus',\n",
       " 'Azure Storage',\n",
       " 'Azure Synapse',\n",
       " 'Azure Synapse Analytics',\n",
       " 'BLE',\n",
       " 'Back-End Development',\n",
       " 'Bamboo',\n",
       " 'Bash',\n",
       " 'Beautiful Soup',\n",
       " 'Big Data',\n",
       " 'BigQuery',\n",
       " 'Bitbucket',\n",
       " 'Bloc',\n",
       " 'Blockchain',\n",
       " 'Bluetooth',\n",
       " 'Bootstrap',\n",
       " 'Business Intelligence',\n",
       " 'C',\n",
       " 'C#',\n",
       " 'C++',\n",
       " 'CI/CD',\n",
       " 'CI/CD Pipelines',\n",
       " 'CMS',\n",
       " 'CRM',\n",
       " 'CSS',\n",
       " 'CSS Modules',\n",
       " 'CSS3',\n",
       " 'Caching',\n",
       " 'Cassandra',\n",
       " 'Causal Inference',\n",
       " 'Celery',\n",
       " 'Chakra UI',\n",
       " 'Chef',\n",
       " 'Chroma',\n",
       " 'CircleCI',\n",
       " 'Clojure',\n",
       " 'Cloud Build',\n",
       " 'Cloud Deployment',\n",
       " 'Cloud Native',\n",
       " 'Cloud Run',\n",
       " 'Cloud Storage',\n",
       " 'CloudFormation',\n",
       " 'CloudWatch',\n",
       " 'Cloudflare',\n",
       " 'Cognitive Search',\n",
       " 'Combinatorial Optimization',\n",
       " 'ComfyUI',\n",
       " 'Communication',\n",
       " 'Compliance',\n",
       " 'Computer Networks',\n",
       " 'Computer Vision',\n",
       " 'Configuration Management',\n",
       " 'Confluence',\n",
       " 'Container Orchestration',\n",
       " 'Containerization',\n",
       " 'Content Evaluation',\n",
       " 'Continuous Delivery',\n",
       " 'Continuous Deployment',\n",
       " 'Continuous Integration',\n",
       " 'Control Theory',\n",
       " 'Cosmos DB',\n",
       " 'Cubit',\n",
       " 'Cypress',\n",
       " 'DAX',\n",
       " 'DNS',\n",
       " 'Dagster',\n",
       " 'Dart',\n",
       " 'Dashboard',\n",
       " 'Dashboarding',\n",
       " 'Dask',\n",
       " 'Data Analysis',\n",
       " 'Data Analytics',\n",
       " 'Data Backup',\n",
       " 'Data Communications',\n",
       " 'Data Factory',\n",
       " 'Data Flows',\n",
       " 'Data Governance',\n",
       " 'Data Integration',\n",
       " 'Data Lake',\n",
       " 'Data Management',\n",
       " 'Data Mesh',\n",
       " 'Data Mining',\n",
       " 'Data Modeling',\n",
       " 'Data Orchestration',\n",
       " 'Data Pipeline',\n",
       " 'Data Pipelines',\n",
       " 'Data Processing',\n",
       " 'Data Quality',\n",
       " 'Data Structures',\n",
       " 'Data Transformation',\n",
       " 'Data Validation',\n",
       " 'Data Visualization',\n",
       " 'Data Warehousing',\n",
       " 'DataBricks',\n",
       " 'DataHub',\n",
       " 'Database Design',\n",
       " 'Database Systems',\n",
       " 'Databricks',\n",
       " 'Datadog',\n",
       " 'Dataflow',\n",
       " 'Deep Learning',\n",
       " 'Delta Lake',\n",
       " 'Design Patterns',\n",
       " 'DevOps',\n",
       " 'DevSecOps',\n",
       " 'Disaster Recovery',\n",
       " 'Distributed Systems',\n",
       " 'Django',\n",
       " 'Django REST Framework',\n",
       " 'Docker',\n",
       " 'Documentation',\n",
       " 'DynamoDB',\n",
       " 'Dynatrace',\n",
       " 'EAS',\n",
       " 'EC2',\n",
       " 'ECS',\n",
       " 'ELK',\n",
       " 'ELK Stack',\n",
       " 'ELT',\n",
       " 'ERP',\n",
       " 'ETL',\n",
       " 'ETL Pipeline',\n",
       " 'ETL Pipelines',\n",
       " 'Edge Computing',\n",
       " 'Edge Devices',\n",
       " 'Elastic Search',\n",
       " 'Elasticsearch',\n",
       " 'Embedded Firmware',\n",
       " 'Embedded Systems',\n",
       " 'Embeddings',\n",
       " 'Encryption',\n",
       " 'Entity Framework Core',\n",
       " 'Excel',\n",
       " 'ExpessJS',\n",
       " 'Expo',\n",
       " 'Express',\n",
       " 'FastAPI',\n",
       " 'Feature Engineering',\n",
       " 'Feature Stores',\n",
       " 'Figma',\n",
       " 'Firewalls',\n",
       " 'Flask',\n",
       " 'Flink',\n",
       " 'Flipper',\n",
       " 'Flutter',\n",
       " 'Framer Motion',\n",
       " 'Frameworks',\n",
       " 'Front-End Development',\n",
       " 'Frontend Architecture',\n",
       " 'Fuzzy Logic',\n",
       " 'GAN',\n",
       " 'GCP',\n",
       " 'GIT',\n",
       " 'GKE',\n",
       " 'GPT-3',\n",
       " 'GPT-4',\n",
       " 'GSAP',\n",
       " 'GStreamer',\n",
       " 'Gearman',\n",
       " 'GenAI',\n",
       " 'Generative AI',\n",
       " 'Geometric Algorithms',\n",
       " 'Git',\n",
       " 'GitHub',\n",
       " 'GitHub Actions',\n",
       " 'GitLab',\n",
       " 'GitLab CI',\n",
       " 'GitOps',\n",
       " 'Glue',\n",
       " 'Go',\n",
       " 'Golang',\n",
       " 'Google Analytics',\n",
       " 'Google Cloud',\n",
       " 'Google Cloud Platform',\n",
       " 'Google Data Studio',\n",
       " 'Google Gemini',\n",
       " 'Google Sheets',\n",
       " 'Governance',\n",
       " 'Grafana',\n",
       " 'Grafana Agent',\n",
       " 'Graph Algorithms',\n",
       " 'GraphDB',\n",
       " 'GraphQL',\n",
       " 'Groovy',\n",
       " 'Guidance Algorithms',\n",
       " 'HBase',\n",
       " 'HDFS',\n",
       " 'HTML',\n",
       " 'HTML5',\n",
       " 'Hadoop',\n",
       " 'Helm',\n",
       " 'High-Level Design',\n",
       " 'Hive',\n",
       " 'Hugging Face',\n",
       " 'HuggingFace',\n",
       " 'IAM',\n",
       " 'ISO 27001',\n",
       " 'IaC',\n",
       " 'Information Risk Management',\n",
       " 'Information Security',\n",
       " 'Infrastructure as Code',\n",
       " 'Innovation',\n",
       " 'Integration Testing',\n",
       " 'IoT',\n",
       " 'Istio',\n",
       " 'J2EE',\n",
       " 'JIRA',\n",
       " 'JSON',\n",
       " 'Java',\n",
       " 'JavaScript',\n",
       " 'Jax',\n",
       " 'Jenkins',\n",
       " 'Jest',\n",
       " 'Jotai',\n",
       " 'Jupyter',\n",
       " 'KMS',\n",
       " 'Kafka',\n",
       " 'Kanban',\n",
       " 'Keras',\n",
       " 'Keystatic CMS',\n",
       " 'Kinesis',\n",
       " 'Kotlin',\n",
       " 'KubeFlow',\n",
       " 'Kubeflow',\n",
       " 'Kubernetes',\n",
       " 'LLM',\n",
       " 'LLMOps',\n",
       " 'LLMs',\n",
       " 'Lambda',\n",
       " 'LangChain',\n",
       " 'LangGraph',\n",
       " 'LangSmith',\n",
       " 'Langchain',\n",
       " 'Langraft',\n",
       " 'Large Language Model',\n",
       " 'Large Language Models',\n",
       " 'Linux',\n",
       " 'LlamaIndex',\n",
       " 'LoRA',\n",
       " 'Load Balancing',\n",
       " 'Logging',\n",
       " 'Loki',\n",
       " 'LookML',\n",
       " 'Looker',\n",
       " 'Low-Level Design',\n",
       " 'Luigi',\n",
       " 'MATLAB',\n",
       " 'MCP',\n",
       " 'ML',\n",
       " 'ML Workflows',\n",
       " 'MLI',\n",
       " 'MLOps',\n",
       " 'MLflow',\n",
       " 'MPC',\n",
       " 'MQTT',\n",
       " 'MS SQL Server',\n",
       " 'MSK',\n",
       " 'MVC',\n",
       " 'Machine Learning',\n",
       " 'Macros',\n",
       " 'MapReduce',\n",
       " 'MariaDB',\n",
       " 'Markdown',\n",
       " 'Material UI',\n",
       " 'Matplotlib',\n",
       " 'Memgraph',\n",
       " 'Metrics',\n",
       " 'Microservices',\n",
       " 'Microsoft Azure',\n",
       " 'Microsoft Fabric',\n",
       " 'Model Inference',\n",
       " 'Model Monitoring',\n",
       " 'Model Operationalization',\n",
       " 'Model Pruning',\n",
       " 'Model Serving',\n",
       " 'Modern Web Technologies',\n",
       " 'MongoDB',\n",
       " 'Monitoring',\n",
       " 'Multivariable Control',\n",
       " 'MySQL',\n",
       " 'NER',\n",
       " 'NLP',\n",
       " 'NLTK',\n",
       " 'NUnit',\n",
       " 'NVIDIA DeepStream',\n",
       " 'NVIDIA Jetson',\n",
       " 'Natural Language Processing',\n",
       " 'Neo4j',\n",
       " 'Networking',\n",
       " 'Neural Networks',\n",
       " 'Next.js',\n",
       " 'Nginx',\n",
       " 'NoSQL',\n",
       " 'NoSQL Databases',\n",
       " 'Node.js',\n",
       " 'NumPy',\n",
       " 'OAuth',\n",
       " 'OAuth 2.0',\n",
       " 'ONNX',\n",
       " 'OneLake',\n",
       " 'OpenAI',\n",
       " 'OpenAPI',\n",
       " 'OpenShift',\n",
       " 'OpenTelemetry',\n",
       " 'Opsgenie',\n",
       " 'Oracle Cloud',\n",
       " 'Oracle Cloud AI',\n",
       " 'Orchestrator',\n",
       " 'Ovation DCS',\n",
       " 'PHP',\n",
       " 'PID',\n",
       " 'PRISM',\n",
       " 'Pandas',\n",
       " 'Pinecone',\n",
       " 'Playwright',\n",
       " 'Plotly',\n",
       " 'PostgreSQL',\n",
       " 'Postman',\n",
       " 'Power Apps',\n",
       " 'Power Automate',\n",
       " 'Power BI',\n",
       " 'PowerPoint',\n",
       " 'PowerShell',\n",
       " 'Predictive Maintenance',\n",
       " 'Predictive Modeling',\n",
       " 'Prefect',\n",
       " 'Probabilistic Graphical Models',\n",
       " 'Prometheus',\n",
       " 'Prompt Engineering',\n",
       " 'Pub/Sub',\n",
       " 'Puppet',\n",
       " 'PyAutoGUI',\n",
       " 'PySpark',\n",
       " 'PyTorch',\n",
       " 'Python',\n",
       " 'QA',\n",
       " 'QGIS',\n",
       " 'Qlik Sense',\n",
       " 'Quicksight',\n",
       " 'R',\n",
       " 'R-trees',\n",
       " 'RAG',\n",
       " 'RDS',\n",
       " 'REST API',\n",
       " 'REST APIs',\n",
       " 'RESTful API',\n",
       " 'RESTful APIs',\n",
       " 'RLHF',\n",
       " 'Radar',\n",
       " 'Ray',\n",
       " 'React',\n",
       " 'React Context',\n",
       " 'React Context API',\n",
       " 'React Hooks',\n",
       " 'React Native',\n",
       " 'React Navigation',\n",
       " 'React Query',\n",
       " 'React Testing Library',\n",
       " 'ReactJS',\n",
       " 'Recommendation Systems',\n",
       " 'Redis',\n",
       " 'Redshift',\n",
       " 'Redux',\n",
       " 'Redux Toolkit',\n",
       " 'Reinforcement Learning',\n",
       " 'Reporting',\n",
       " 'Responsive Design',\n",
       " 'Risk and Controls',\n",
       " 'Rollup',\n",
       " 'Route53',\n",
       " 'Ruby',\n",
       " 'Rust',\n",
       " 'RxJS',\n",
       " 'S3',\n",
       " 'SAFe',\n",
       " 'SAP',\n",
       " 'SFTP',\n",
       " 'SIEM',\n",
       " 'SNS',\n",
       " 'SOA',\n",
       " 'SOC 2',\n",
       " 'SQL',\n",
       " 'SQL Server',\n",
       " 'SQS',\n",
       " 'SRE',\n",
       " 'SSH',\n",
       " 'SSIS',\n",
       " 'SVN',\n",
       " 'SageMaker',\n",
       " 'Salesforce',\n",
       " 'Scala',\n",
       " 'Schema Markup',\n",
       " 'Scikit-learn',\n",
       " 'Scripting',\n",
       " 'Scrum',\n",
       " 'Seaborn',\n",
       " 'Security',\n",
       " 'Security Assessments',\n",
       " 'Selenium',\n",
       " 'SendGrid',\n",
       " 'Sensors',\n",
       " 'Sequential Modeling',\n",
       " 'Serverless',\n",
       " 'Service Discovery',\n",
       " 'Service Management',\n",
       " 'Service Mesh',\n",
       " 'ServiceNow',\n",
       " 'SharePoint',\n",
       " 'Shell Scripting',\n",
       " 'Shopify',\n",
       " 'Site Reliability Engineering',\n",
       " 'Smartsheet',\n",
       " 'Snowflake',\n",
       " 'Software Configuration Management',\n",
       " 'Solr',\n",
       " 'Source Control',\n",
       " 'Spark',\n",
       " 'Spark SQL',\n",
       " 'Spark Streaming',\n",
       " 'Spatial Algorithms',\n",
       " 'Splunk',\n",
       " 'Spotfire',\n",
       " 'Spring Boot',\n",
       " 'Starlink',\n",
       " 'Statistical Modeling',\n",
       " 'Statistics',\n",
       " 'Step Functions',\n",
       " 'Stored Procedures',\n",
       " 'Supabase',\n",
       " 'Swagger',\n",
       " 'Swift',\n",
       " 'Symfony',\n",
       " 'Synapse Analytics',\n",
       " 'System Design',\n",
       " 'System Testing',\n",
       " 'T-SQL',\n",
       " 'Tableau',\n",
       " 'Tailwind CSS',\n",
       " 'TailwindCSS',\n",
       " 'Tempo',\n",
       " 'Temporal Graphs',\n",
       " 'Temporal Reasoning',\n",
       " 'TensorFlow',\n",
       " 'TensorRT',\n",
       " 'Terraform',\n",
       " 'Test Automation',\n",
       " 'Test Driven Development',\n",
       " 'Thanos',\n",
       " 'Time Management',\n",
       " 'Time-Series Analysis',\n",
       " 'Time-Series Data',\n",
       " 'Time-Series Forecasting',\n",
       " 'Trade Data',\n",
       " 'Transformer',\n",
       " 'Transformers',\n",
       " 'Turborepo',\n",
       " 'TypeScript',\n",
       " 'UI/UX',\n",
       " 'UML',\n",
       " 'Ubuntu',\n",
       " 'UiPath',\n",
       " 'Unit Testing',\n",
       " 'Unity',\n",
       " 'Unity Catalog',\n",
       " 'VAE',\n",
       " 'VIPER Architecture',\n",
       " 'VR',\n",
       " 'Vault',\n",
       " 'Vector Database',\n",
       " 'Vector Databases',\n",
       " 'Vector Stores',\n",
       " 'Vehicle Dynamics',\n",
       " 'Vercel',\n",
       " 'Version Control',\n",
       " 'Vertex AI',\n",
       " 'Vite',\n",
       " 'Vitest',\n",
       " 'Vue.js',\n",
       " 'Vulnerability Analysis',\n",
       " 'Web API',\n",
       " 'Web Design',\n",
       " 'Web Development',\n",
       " 'Web Frameworks',\n",
       " 'Web Scraping',\n",
       " 'Web3',\n",
       " 'WebSocket',\n",
       " 'Webflow',\n",
       " 'Webpack',\n",
       " 'Wi-Fi',\n",
       " 'Word',\n",
       " 'WordPress',\n",
       " 'Workday',\n",
       " 'XGBoost',\n",
       " 'XR',\n",
       " 'Xcode',\n",
       " 'YAML',\n",
       " 'YOLO',\n",
       " 'dbt',\n",
       " 'gRPC',\n",
       " 'i18n',\n",
       " 'iOS',\n",
       " 'iOS Development',\n",
       " 'k-d trees',\n",
       " 'n8n',\n",
       " 'pgvector',\n",
       " 'pnpm workspaces',\n",
       " 'scikit-learn',\n",
       " 'spaCy',\n",
       " 'xUnit']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a757793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role Key Counts:\n",
      "role_key\n",
      "software_engineer    40\n",
      "ai_ml_engineer       37\n",
      "data_analyst         37\n",
      "devops_engineer      36\n",
      "data_engineer        33\n",
      "web_developer        11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total unique role keys: 6\n"
     ]
    }
   ],
   "source": [
    "# Parse skills column - split by '|' into list\n",
    "df['skills_list'] = df['skills'].apply(lambda x: [s.strip() for s in str(x).split('|')] if pd.notna(x) and x else [])\n",
    "\n",
    "# Check role_key counts\n",
    "print(\"Role Key Counts:\")\n",
    "print(df['role_key'].value_counts())\n",
    "print(f\"\\nTotal unique role keys: {df['role_key'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6473e017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 Raw Skills:\n",
      "Python: 122\n",
      "SQL: 70\n",
      "AWS: 54\n",
      "CI/CD: 51\n",
      "Docker: 42\n",
      "Kubernetes: 37\n",
      "Azure: 37\n",
      "Java: 37\n",
      "JavaScript: 32\n",
      "Git: 30\n",
      "ETL: 30\n",
      "Power BI: 28\n",
      "React: 27\n",
      "Terraform: 26\n",
      "Machine Learning: 24\n",
      "PyTorch: 22\n",
      "TypeScript: 22\n",
      "Data Modeling: 21\n",
      "Go: 21\n",
      "Bash: 21\n",
      "GCP: 21\n",
      "Databricks: 20\n",
      "Jenkins: 20\n",
      "TensorFlow: 19\n",
      "Linux: 19\n",
      "Kafka: 17\n",
      "IaC: 16\n",
      "Airflow: 15\n",
      "GitHub: 15\n",
      "R: 15\n"
     ]
    }
   ],
   "source": [
    "# Get top 30 raw skills\n",
    "from collections import Counter\n",
    "all_skills = []\n",
    "for skills in df['skills_list']:\n",
    "    all_skills.extend(skills)\n",
    "\n",
    "skill_counts = Counter(all_skills)\n",
    "print(\"Top 30 Raw Skills:\")\n",
    "for skill, count in skill_counts.most_common(30):\n",
    "    print(f\"{skill}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e930dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exported mapping template to: ../data/job_skill_to_parent_skill.csv\n",
      "  Total skills: 200\n",
      "  Coverage: 1886 / 2307 skill occurrences (81.8%)\n",
      "\n",
      "First 10 rows of template:\n",
      "  job_skill_norm  count parent_skill\n",
      "0         python    122             \n",
      "1            sql     70             \n",
      "2            aws     54             \n",
      "3          ci/cd     51             \n",
      "4         docker     42             \n",
      "5     kubernetes     37             \n",
      "6          azure     37             \n",
      "7           java     37             \n",
      "8     javascript     32             \n",
      "9            git     31             \n",
      "\n",
      "Please fill in the 'parent_skill' column manually!\n"
     ]
    }
   ],
   "source": [
    "# Export CSV template for manual mapping (top 200 skills)\n",
    "top_200_skills = skill_freq.most_common(200)\n",
    "\n",
    "# Create DataFrame with columns: job_skill_norm, count, parent_skill\n",
    "mapping_template = pd.DataFrame({\n",
    "    'job_skill_norm': [skill for skill, count in top_200_skills],\n",
    "    'count': [count for skill, count in top_200_skills],\n",
    "    'parent_skill': [''] * len(top_200_skills)  # Blank for manual filling\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_path = '../data/job_skill_to_parent_skill.csv'\n",
    "mapping_template.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Exported mapping template to: {output_path}\")\n",
    "print(f\"  Total skills: {len(mapping_template)}\")\n",
    "print(f\"  Coverage: {sum(mapping_template['count'])} / {len(normalized_skills)} skill occurrences ({sum(mapping_template['count'])/len(normalized_skills)*100:.1f}%)\")\n",
    "print(f\"\\nFirst 10 rows of template:\")\n",
    "print(mapping_template.head(10))\n",
    "print(f\"\\nPlease fill in the 'parent_skill' column manually!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4befa817",
   "metadata": {},
   "source": [
    "### Manual Parent Skill Filling\n",
    "\n",
    "**Instructions:**\n",
    "1. Open `../data/job_skill_to_parent_skill.csv` in Excel or a text editor\n",
    "2. Fill in the `parent_skill` column for each job skill\n",
    "3. Use parent skills from `../data/skill_group_map.csv` (27 categories)\n",
    "4. Save the file after completing the mappings\n",
    "5. Run the validation cell below to check your progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7007938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping validation table: top 30 skills with their mapped parent_skill\n",
    "print(\"\\nMAPPING VALIDATION: Top 30 Job Skills → Parent Skills\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the mapping file\n",
    "validation_mapping = pd.read_csv('../data/job_skill_to_parent_skill.csv')\n",
    "\n",
    "# Create validation dataframe for top 30 skills\n",
    "validation_data = []\n",
    "for idx, (skill, count) in enumerate(top_30_skills, 1):\n",
    "    # Find the parent_skill mapping\n",
    "    mapping_row = validation_mapping[validation_mapping['job_skill_norm'] == skill]\n",
    "    \n",
    "    if len(mapping_row) > 0:\n",
    "        parent = mapping_row.iloc[0]['parent_skill']\n",
    "        parent_display = parent if pd.notna(parent) and parent != '' else '❌ NOT MAPPED'\n",
    "    else:\n",
    "        parent_display = '⚠️ NOT IN TEMPLATE'\n",
    "    \n",
    "    validation_data.append({\n",
    "        'Rank': idx,\n",
    "        'Job Skill': skill,\n",
    "        'Count': count,\n",
    "        'Parent Skill': parent_display\n",
    "    })\n",
    "\n",
    "# Create and display validation DataFrame\n",
    "validation_df = pd.DataFrame(validation_data)\n",
    "print(validation_df.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "mapped_count = sum(1 for item in validation_data if '❌' not in item['Parent Skill'] and '⚠️' not in item['Parent Skill'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Mapping Coverage: {mapped_count}/{len(top_30_skills)} ({mapped_count/len(top_30_skills)*100:.1f}%) of top 30 skills are mapped\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show the validation table\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a30d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 skills per role_key (normalized)\n",
    "print(\"\\nTOP 10 SKILLS PER ROLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for role in sorted(df['role_key'].unique()):\n",
    "    role_df = df[df['role_key'] == role]\n",
    "    \n",
    "    # Collect all normalized skills for this role\n",
    "    role_skills = []\n",
    "    for skills in role_df['skills_list']:\n",
    "        for skill in skills:\n",
    "            skill_norm = skill.strip().lower()\n",
    "            if skill_norm:\n",
    "                role_skills.append(skill_norm)\n",
    "    \n",
    "    # Count and display top 10\n",
    "    role_skill_freq = Counter(role_skills)\n",
    "    total_jobs = len(role_df)\n",
    "    \n",
    "    print(f\"\\n{role} ({total_jobs} jobs):\")\n",
    "    for idx, (skill, count) in enumerate(role_skill_freq.most_common(10), 1):\n",
    "        pct = (count / total_jobs) * 100\n",
    "        print(f\"  {idx:2d}. {skill:45s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 normalized job skills with counts\n",
    "print(\"TOP 30 IN-DEMAND JOB SKILLS\")\n",
    "print(\"=\"*80)\n",
    "top_30_skills = skill_freq.most_common(30)\n",
    "for idx, (skill, count) in enumerate(top_30_skills, 1):\n",
    "    print(f\"{idx:2d}. {skill:50s} {count:5d} occurrences\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ae315",
   "metadata": {},
   "source": [
    "## Market Skill Summary\n",
    "\n",
    "Analyze the most in-demand job skills and validate mapping quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 20 skills per role_key (optional)\n",
    "print(\"\\nTop 20 Skills per Role:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for role in sorted(df['role_key'].unique()):\n",
    "    role_df = df[df['role_key'] == role]\n",
    "    \n",
    "    # Collect all normalized skills for this role\n",
    "    role_skills = []\n",
    "    for skills in role_df['skills_list']:\n",
    "        for skill in skills:\n",
    "            skill_norm = skill.strip().lower()\n",
    "            if skill_norm:\n",
    "                role_skills.append(skill_norm)\n",
    "    \n",
    "    # Count and display top 20\n",
    "    role_skill_freq = Counter(role_skills)\n",
    "    print(f\"\\n{role} (total: {len(role_skills)} skills, unique: {len(role_skill_freq)}):\")\n",
    "    for idx, (skill, count) in enumerate(role_skill_freq.most_common(20), 1):\n",
    "        print(f\"  {idx:2d}. {skill:45s} ({count:3d})\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b26ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 50 skills with counts\n",
    "print(\"Top 50 Normalized Skills:\")\n",
    "print(\"=\"*80)\n",
    "for idx, (skill, count) in enumerate(skill_freq.most_common(50), 1):\n",
    "    print(f\"{idx:2d}. {skill:50s} ({count:4d} occurrences)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build normalized frequency table\n",
    "from collections import Counter\n",
    "\n",
    "# Normalize skills: strip spaces, lowercase, remove empty strings\n",
    "normalized_skills = []\n",
    "for skills in df['skills_list']:\n",
    "    for skill in skills:\n",
    "        skill_norm = skill.strip().lower()\n",
    "        if skill_norm:  # Remove empty strings\n",
    "            normalized_skills.append(skill_norm)\n",
    "\n",
    "# Count frequencies\n",
    "skill_freq = Counter(normalized_skills)\n",
    "print(f\"Total skill occurrences: {len(normalized_skills)}\")\n",
    "print(f\"Unique normalized skills: {len(skill_freq)}\")\n",
    "print(f\"\\nSkill frequency distribution:\")\n",
    "print(f\"  Skills appearing once: {sum(1 for count in skill_freq.values() if count == 1)}\")\n",
    "print(f\"  Skills appearing 2-5 times: {sum(1 for count in skill_freq.values() if 2 <= count <= 5)}\")\n",
    "print(f\"  Skills appearing 6+ times: {sum(1 for count in skill_freq.values() if count >= 6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664fcf49",
   "metadata": {},
   "source": [
    "## Job Skill Vocabulary Analysis\n",
    "\n",
    "Build a normalized frequency table of all raw job skills and export a mapping template for manual parent skill assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78891fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load job skill mapping (normalized job skills -> parent skills)\n",
    "job_mapping_df = pd.read_csv('../data/job_skill_to_parent_skill.csv')\n",
    "print(f\"Loaded {len(job_mapping_df)} job skill mappings\")\n",
    "\n",
    "# Drop rows where parent_skill is blank\n",
    "job_mapping_df = job_mapping_df[job_mapping_df['parent_skill'].notna() & (job_mapping_df['parent_skill'] != '')]\n",
    "print(f\"After filtering blank parent_skill: {len(job_mapping_df)} mappings\")\n",
    "\n",
    "# Create mapping dictionary: job_skill_norm -> parent_skill\n",
    "job_skill_to_parent = dict(zip(job_mapping_df['job_skill_norm'], job_mapping_df['parent_skill']))\n",
    "print(f\"\\nUnique parent skills mapped: {job_mapping_df['parent_skill'].nunique()}\")\n",
    "\n",
    "# Load original skill_group_map to get all 27 parent skills\n",
    "skill_map_df = pd.read_csv('../data/skill_group_map.csv')\n",
    "all_parent_skills = sorted(skill_map_df['parent_skill'].unique())\n",
    "print(f\"Total parent skills (from skill_group_map): {len(all_parent_skills)}\")\n",
    "print(f\"\\nParent skills: {all_parent_skills}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature matrix X using job skill mappings\n",
    "print(f\"Creating feature matrix with {len(all_parent_skills)} parent skill columns...\")\n",
    "\n",
    "# Initialize feature matrix\n",
    "X_dict = {parent_skill: [] for parent_skill in all_parent_skills}\n",
    "\n",
    "# For each job, map normalized skills to parent skills\n",
    "for idx, row in df.iterrows():\n",
    "    job_skills = row['skills_list']\n",
    "    \n",
    "    # Track which parent skills are active for this job\n",
    "    active_parents = set()\n",
    "    \n",
    "    # Normalize each job skill and map to parent skill\n",
    "    for job_skill in job_skills:\n",
    "        skill_norm = job_skill.strip().lower()\n",
    "        if skill_norm in job_skill_to_parent:\n",
    "            parent_skill = job_skill_to_parent[skill_norm]\n",
    "            active_parents.add(parent_skill)\n",
    "    \n",
    "    # Set binary features for each parent skill\n",
    "    for parent_skill in all_parent_skills:\n",
    "        X_dict[parent_skill].append(1 if parent_skill in active_parents else 0)\n",
    "\n",
    "# Create DataFrame\n",
    "X = pd.DataFrame(X_dict)\n",
    "y = df['role_key']\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nFeature matrix (first 5 rows, first 10 columns):\")\n",
    "print(X.iloc[:5, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50028f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage checks and statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COVERAGE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Non-zero feature columns count\n",
    "non_zero_cols = (X.sum(axis=0) > 0).sum()\n",
    "print(f\"\\nNon-zero feature columns: {non_zero_cols} / {len(all_parent_skills)}\")\n",
    "\n",
    "# Jobs with at least 1 mapped parent skill\n",
    "jobs_with_skills = (X.sum(axis=1) > 0).sum()\n",
    "percent_with_skills = (jobs_with_skills / len(X)) * 100\n",
    "print(f\"Jobs with ≥1 mapped parent skill: {jobs_with_skills} / {len(X)} ({percent_with_skills:.1f}%)\")\n",
    "\n",
    "# Average mapped parent skills per job\n",
    "avg_skills_per_job = X.sum(axis=1).mean()\n",
    "print(f\"Average mapped parent skills per job: {avg_skills_per_job:.2f}\")\n",
    "\n",
    "# Top 10 active parent skills by total count\n",
    "parent_skill_counts = X.sum(axis=0).sort_values(ascending=False)\n",
    "print(f\"\\nTop 10 active parent skills by total count:\")\n",
    "for idx, (skill, count) in enumerate(parent_skill_counts.head(10).items(), 1):\n",
    "    print(f\"  {idx:2d}. {skill:50s} ({int(count):4d} jobs)\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine X and y into final dataset\n",
    "final_df = X.copy()\n",
    "final_df['role_key'] = y.values\n",
    "\n",
    "# Save to CSV\n",
    "output_path = '../data/job_parent_skill_features.csv'\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(f\"\\n✓ Saved feature matrix to: {output_path}\")\n",
    "print(f\"  - Shape: {final_df.shape}\")\n",
    "print(f\"  - Columns: {len(final_df.columns)} ({len(all_parent_skills)} features + 1 target)\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b185f",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation\n",
    "\n",
    "Train and evaluate multiclass classification models to predict role_key from parent skill features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e276dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (numpy<2.4 for SHAP/numba compatibility)\n",
    "# Note: After running this cell, restart the kernel before continuing\n",
    "%pip install --force-reinstall \"numpy<2.4\" scikit-learn shap joblib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import shap\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "print(\"✓ Libraries imported and models directory ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac94721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature matrix\n",
    "data = pd.read_csv('../data/job_parent_skill_features.csv')\n",
    "print(f\"Loaded data: {data.shape}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('role_key', axis=1)\n",
    "y = data['role_key']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target classes: {label_encoder.classes_}\")\n",
    "print(f\"Class distribution:\\n{pd.Series(y).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation functions\n",
    "def top_k_accuracy(y_true, y_proba, k=3):\n",
    "    \"\"\"Calculate top-k accuracy\"\"\"\n",
    "    top_k_preds = np.argsort(y_proba, axis=1)[:, -k:]\n",
    "    correct = sum([y_true[i] in top_k_preds[i] for i in range(len(y_true))])\n",
    "    return correct / len(y_true)\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Evaluate a model and return metrics\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_proba = model.predict_proba(X_val)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    top3 = top_k_accuracy(y_val, y_proba, k=3)\n",
    "    \n",
    "    return acc, f1, top3\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'LogisticRegression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=5000, solver='lbfgs', class_weight='balanced', random_state=42))\n",
    "    ]),\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=400, \n",
    "        random_state=42, \n",
    "        class_weight='balanced_subsample'\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Models defined:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47928efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold stratified cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    fold_results = {'accuracy': [], 'macro_f1': [], 'top3_acc': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_encoded), 1):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "        \n",
    "        acc, f1, top3 = evaluate_model(model, X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        fold_results['accuracy'].append(acc)\n",
    "        fold_results['macro_f1'].append(f1)\n",
    "        fold_results['top3_acc'].append(top3)\n",
    "        \n",
    "        print(f\"  Fold {fold}: Acc={acc:.4f}, F1={f1:.4f}, Top-3={top3:.4f}\")\n",
    "    \n",
    "    # Calculate means\n",
    "    mean_acc = np.mean(fold_results['accuracy'])\n",
    "    mean_f1 = np.mean(fold_results['macro_f1'])\n",
    "    mean_top3 = np.mean(fold_results['top3_acc'])\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'mean_accuracy': mean_acc,\n",
    "        'mean_macro_f1': mean_f1,\n",
    "        'mean_top3_acc': mean_top3,\n",
    "        'fold_results': fold_results\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  MEAN RESULTS:\")\n",
    "    print(f\"    Accuracy:  {mean_acc:.4f} ± {np.std(fold_results['accuracy']):.4f}\")\n",
    "    print(f\"    Macro F1:  {mean_f1:.4f} ± {np.std(fold_results['macro_f1']):.4f}\")\n",
    "    print(f\"    Top-3 Acc: {mean_top3:.4f} ± {np.std(fold_results['top3_acc']):.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Cross-validation complete!\")\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc25429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on Macro F1, break ties with Top-3 accuracy\n",
    "best_model_name = None\n",
    "best_f1 = -1\n",
    "best_top3 = -1\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    if metrics['mean_macro_f1'] > best_f1:\n",
    "        best_f1 = metrics['mean_macro_f1']\n",
    "        best_top3 = metrics['mean_top3_acc']\n",
    "        best_model_name = model_name\n",
    "    elif metrics['mean_macro_f1'] == best_f1 and metrics['mean_top3_acc'] > best_top3:\n",
    "        best_top3 = metrics['mean_top3_acc']\n",
    "        best_model_name = model_name\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print('='*60)\n",
    "print(f\"  Mean Accuracy:  {results[best_model_name]['mean_accuracy']:.4f}\")\n",
    "print(f\"  Mean Macro F1:  {results[best_model_name]['mean_macro_f1']:.4f}\")\n",
    "print(f\"  Mean Top-3 Acc: {results[best_model_name]['mean_top3_acc']:.4f}\")\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab834dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best model on full dataset\n",
    "print(f\"\\nTraining {best_model_name} on full dataset...\")\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X, y_encoded)\n",
    "\n",
    "# Save model\n",
    "model_path = '../models/role_model.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"✓ Saved model to: {model_path}\")\n",
    "\n",
    "# Save feature columns\n",
    "feature_columns = X.columns.tolist()\n",
    "with open('../models/feature_columns.json', 'w') as f:\n",
    "    json.dump(feature_columns, f, indent=2)\n",
    "print(f\"✓ Saved {len(feature_columns)} feature columns to: ../models/feature_columns.json\")\n",
    "\n",
    "# Save role labels mapping\n",
    "role_labels = {int(i): label for i, label in enumerate(label_encoder.classes_)}\n",
    "with open('../models/role_labels.json', 'w') as f:\n",
    "    json.dump(role_labels, f, indent=2)\n",
    "print(f\"✓ Saved role labels mapping to: ../models/role_labels.json\")\n",
    "print(f\"  Role mapping: {role_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc00199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute role prototypes for skill gap analysis\n",
    "# Group by role_key and compute mean of each feature\n",
    "role_prototypes = data.groupby('role_key')[feature_columns].mean()\n",
    "\n",
    "# Save role prototypes\n",
    "prototypes_path = '../models/role_prototypes.csv'\n",
    "role_prototypes.to_csv(prototypes_path)\n",
    "print(f\"\\n✓ Saved role prototypes to: {prototypes_path}\")\n",
    "print(f\"  Shape: {role_prototypes.shape}\")\n",
    "print(f\"\\nRole Prototypes (sample):\")\n",
    "print(role_prototypes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72f8d2",
   "metadata": {},
   "source": [
    "## SHAP Analysis for Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer based on model type\n",
    "print(f\"\\nCreating SHAP explainer for {best_model_name}...\")\n",
    "\n",
    "if best_model_name == 'LogisticRegression':\n",
    "    # For LogisticRegression pipeline, need to transform data first\n",
    "    X_transformed = best_model.named_steps['scaler'].transform(X)\n",
    "    explainer = shap.LinearExplainer(best_model.named_steps['clf'], X_transformed)\n",
    "    shap_values = explainer.shap_values(X_transformed)\n",
    "else:  # RandomForest\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "print(f\"✓ SHAP values computed\")\n",
    "print(f\"  SHAP values shape: {np.array(shap_values).shape if isinstance(shap_values, list) else shap_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ebc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute global importance: mean(abs(shap_values)) per feature and class\n",
    "print(\"\\nComputing global SHAP importance...\")\n",
    "\n",
    "# Handle different SHAP value formats\n",
    "# TreeExplainer returns 3D array: (samples, features, classes)\n",
    "# LinearExplainer returns list of 2D arrays: [(samples, features) for each class]\n",
    "if isinstance(shap_values, list):\n",
    "    # List format from LinearExplainer\n",
    "    num_classes = len(shap_values)\n",
    "    global_importance_data = []\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        class_shap = shap_values[class_idx]\n",
    "        mean_abs_shap = np.abs(class_shap).mean(axis=0)\n",
    "        \n",
    "        for feat_idx, feat_name in enumerate(feature_columns):\n",
    "            global_importance_data.append({\n",
    "                'role_key': label_encoder.classes_[class_idx],\n",
    "                'feature': feat_name,\n",
    "                'mean_abs_shap': float(mean_abs_shap[feat_idx])\n",
    "            })\n",
    "elif len(shap_values.shape) == 3:\n",
    "    # 3D array format from TreeExplainer: (samples, features, classes)\n",
    "    num_classes = shap_values.shape[2]\n",
    "    global_importance_data = []\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # Extract SHAP values for this class across all samples and features\n",
    "        class_shap = shap_values[:, :, class_idx]\n",
    "        mean_abs_shap = np.abs(class_shap).mean(axis=0)\n",
    "        \n",
    "        for feat_idx, feat_name in enumerate(feature_columns):\n",
    "            global_importance_data.append({\n",
    "                'role_key': label_encoder.classes_[class_idx],\n",
    "                'feature': feat_name,\n",
    "                'mean_abs_shap': float(mean_abs_shap[feat_idx])\n",
    "            })\n",
    "else:\n",
    "    # 2D array for binary classification\n",
    "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "    global_importance_data = []\n",
    "    \n",
    "    for feat_idx, feat_name in enumerate(feature_columns):\n",
    "        global_importance_data.append({\n",
    "            'role_key': 'all',\n",
    "            'feature': feat_name,\n",
    "            'mean_abs_shap': float(mean_abs_shap[feat_idx])\n",
    "        })\n",
    "\n",
    "# Save global importance\n",
    "global_importance_df = pd.DataFrame(global_importance_data)\n",
    "global_importance_path = '../models/shap_global_importance.csv'\n",
    "global_importance_df.to_csv(global_importance_path, index=False)\n",
    "print(f\"✓ Saved global SHAP importance to: {global_importance_path}\")\n",
    "\n",
    "# Show top features per class\n",
    "print(\"\\nTop 5 features per role (by mean absolute SHAP):\")\n",
    "for role in label_encoder.classes_:\n",
    "    role_data = global_importance_df[global_importance_df['role_key'] == role].nlargest(5, 'mean_abs_shap')\n",
    "    print(f\"\\n  {role}:\")\n",
    "    for _, row in role_data.iterrows():\n",
    "        print(f\"    {row['feature']}: {row['mean_abs_shap']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute local explanation for one example\n",
    "print(\"\\nComputing local SHAP explanation for sample instance...\")\n",
    "\n",
    "# Pick first instance\n",
    "sample_idx = 0\n",
    "sample_X = X.iloc[sample_idx:sample_idx+1]\n",
    "sample_role = y.iloc[sample_idx]\n",
    "true_class_idx = y_encoded[sample_idx]\n",
    "\n",
    "# Get SHAP values for this sample\n",
    "if best_model_name == 'LogisticRegression':\n",
    "    sample_X_transformed = best_model.named_steps['scaler'].transform(sample_X)\n",
    "    if isinstance(shap_values, list):\n",
    "        # List format: get SHAP values for the true class\n",
    "        sample_shap = shap_values[true_class_idx][sample_idx]\n",
    "    else:\n",
    "        sample_shap = shap_values[sample_idx]\n",
    "else:  # RandomForest\n",
    "    if isinstance(shap_values, list):\n",
    "        # List format: get SHAP values for the true class\n",
    "        sample_shap = shap_values[true_class_idx][sample_idx]\n",
    "    elif len(shap_values.shape) == 3:\n",
    "        # 3D array format: extract for this sample and true class\n",
    "        sample_shap = shap_values[sample_idx, :, true_class_idx]\n",
    "    else:\n",
    "        # 2D format\n",
    "        sample_shap = shap_values[sample_idx]\n",
    "\n",
    "# Create DataFrame with features and SHAP values\n",
    "local_explanation = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'feature_value': sample_X.values[0],\n",
    "    'shap_value': sample_shap\n",
    "})\n",
    "\n",
    "# Sort by absolute SHAP value\n",
    "local_explanation['abs_shap'] = np.abs(local_explanation['shap_value'])\n",
    "local_explanation = local_explanation.sort_values('abs_shap', ascending=False)\n",
    "\n",
    "# Get top 10 positive and negative\n",
    "top_positive = local_explanation[local_explanation['shap_value'] > 0].head(10)\n",
    "top_negative = local_explanation[local_explanation['shap_value'] < 0].head(10)\n",
    "\n",
    "local_example = pd.concat([top_positive, top_negative])\n",
    "local_example['sample_role'] = sample_role\n",
    "\n",
    "# Save local example\n",
    "local_path = '../models/shap_local_example.csv'\n",
    "local_example.to_csv(local_path, index=False)\n",
    "print(f\"✓ Saved local SHAP example to: {local_path}\")\n",
    "print(f\"  Sample role: {sample_role}\")\n",
    "print(f\"\\nTop contributors (positive):\")\n",
    "print(top_positive[['feature', 'feature_value', 'shap_value']].head())\n",
    "print(f\"\\nTop contributors (negative):\")\n",
    "print(top_negative[['feature', 'feature_value', 'shap_value']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ebbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLETE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n✓ Models Trained: {len(models)}\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Accuracy:  {metrics['mean_accuracy']:.4f}\")\n",
    "    print(f\"    Macro F1:  {metrics['mean_macro_f1']:.4f}\")\n",
    "    print(f\"    Top-3 Acc: {metrics['mean_top3_acc']:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Selected Best Model: {best_model_name}\")\n",
    "print(f\"\\n✓ Artifacts Saved:\")\n",
    "print(f\"    - ../models/role_model.pkl\")\n",
    "print(f\"    - ../models/feature_columns.json\")\n",
    "print(f\"    - ../models/role_labels.json\")\n",
    "print(f\"    - ../models/role_prototypes.csv\")\n",
    "print(f\"    - ../models/shap_global_importance.csv\")\n",
    "print(f\"    - ../models/shap_local_example.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training and evaluation complete! 🎉\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
